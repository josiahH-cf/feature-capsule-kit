#!/usr/bin/env python3
# Generated by infrastructure builder – do not edit by hand; re-run builder to update.
"""
Python verifier/packager for a single feature capsule.
Mirrors tools/final_bundle/verify_and_package.sh behavior with a modular design and robust fallbacks.
This file is created for future operational use; the builder does not execute it now.
"""
from __future__ import annotations
import argparse
import datetime as dt
import hashlib
import json
import os
import re
import shutil
import subprocess
from pathlib import Path

ROOT = Path(__file__).resolve().parents[2]

REQUIRED_DOCS = [
    "vision.md",
    "exploration.md",
    "intent_card.md",
    "output_contract.schema.json",
    "action_budget.md",
    "concurrency_model.md",
    "sync_policies.md",
    "reference_set.md",
    "evaluation_and_tripwires.md",
    "observability_slos.md",
    "manual_tests.md",
    "runtime_concurrency_tests.md",
    "meta_prompts.md",
    "phase_transition.md",
    "CHANGELOG.md",
]


def eprint(*a):
    print(*a, flush=True)


def kebab_ok(s: str) -> bool:
    return bool(re.match(r"^[a-z][a-z0-9]*(?:-[a-z0-9]+)*$", s))


def sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def git_commit() -> str:
    try:
        return subprocess.check_output(["git", "rev-parse", "HEAD"], cwd=ROOT).decode().strip()
    except Exception:
        return "unknown"


def utc_date() -> str:
    try:
        return dt.datetime.utcnow().strftime("%Y%m%d")
    except Exception:
        return "unknown"


def utc_iso() -> str:
    return dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")


def run_validator(feature_id: str) -> str:
    val = ROOT / "capsule" / "reports" / "validation" / "validate_all.sh"
    if not val.exists():
        return ""
    env = os.environ.copy()
    env["FEATURE_ID"] = feature_id
    try:
        out = subprocess.check_output(["bash", str(val)], env=env, cwd=ROOT, stderr=subprocess.STDOUT)
        return out.decode("utf-8", errors="replace")
    except subprocess.CalledProcessError as e:
        return e.output.decode("utf-8", errors="replace")


def parse_args() -> argparse.Namespace:
    ap = argparse.ArgumentParser()
    ap.add_argument("feature_id")
    ap.add_argument("--allow-gt-1600-tokens", dest="allow_tokens", choices=["yes", "no"], default="no")
    return ap.parse_args()


def stop(reason: str, need: str, paths: str):
    print(f"STOP: {reason}")
    print(f"NEED: {need}")
    print(f"PATHS: {paths}")


def verify_headers(feature_dir: Path, feature_id: str) -> bool:
    urn_re = re.compile(rf"^urn:automatr:schema:capsule:{re.escape(feature_id)}:[a-z0-9_.-]+:v\d+@[^\s]+$")
    ok = True
    for md in list(feature_dir.glob("*.md")) + list((feature_dir / "reports").glob("*.md")):
        try:
            head = md.read_text(encoding="utf-8").splitlines()[:20]
        except Exception:
            continue
        if not any(l.startswith("feature_id:") for l in head):
            continue
        keys = {k.split(":", 1)[0].strip() for k in head if ":" in k}
        if not {"feature_id", "doc_type", "schema_ref", "version", "updated"}.issubset(keys):
            ok = False
            break
        sref = next((l.split(":", 1)[1].strip() for l in head if l.startswith("schema_ref:")), "")
        if not urn_re.match(sref):
            ok = False
            break
    return ok


def acceptance_vs_required(feature_dir: Path) -> bool:
    schema = json.loads((feature_dir / "output_contract.schema.json").read_text("utf-8"))
    req = schema.get("required") or []
    if not req:
        return True
    ic = (feature_dir / "intent_card.md").read_text("utf-8", errors="replace")
    # Find mapping section
    mapped = set()
    in_map = False
    for ln in ic.splitlines():
        if ln.strip().lower().startswith("## checklist") and "schema" in ln.lower():
            in_map = True
            continue
        if in_map and ln.startswith("## "):
            break
        if in_map and "|" in ln and not ln.strip().lower().startswith("id |"):
            cols = [c.strip(" `") for c in ln.split("|")]
            if len(cols) >= 3:
                mapped.add(cols[2])
    return all(k in mapped for k in req)


def concurrency_tuple_ok(feature_dir: Path) -> bool:
    if "## Concurrency Targets" not in (feature_dir / "intent_card.md").read_text("utf-8", errors="replace"):
        return False
    if "## Concurrency Budget" not in (feature_dir / "action_budget.md").read_text("utf-8", errors="replace"):
        return False
    schema = json.loads((feature_dir / "output_contract.schema.json").read_text("utf-8"))
    ct = schema.get("concurrency_targets") or {}
    return all(k in ct for k in ("throughput_rps", "latency_ms", "error_budget_pct", "window_days")) and all(
        k in ct.get("latency_ms", {}) for k in ("p50", "p95", "p99")
    )


def leakage_ok(feature_dir: Path) -> bool:
    pats = [re.compile(p, re.I) for p in [
        r"\bYou are an? (autonomous|AI|model)\b",
        r"^Purpose$",
        r"^Template$",
        r"You are generating scaffolding documents only",
    ]]
    for md in list(feature_dir.glob("*.md")) + list((feature_dir / "reports").glob("*.md")):
        text = md.read_text("utf-8", errors="replace")
        if any(p.search(text) for p in pats):
            return False
    return True


def unknowns_policy_ok(feature_dir: Path) -> tuple[bool, list[dict]]:
    high_rows: list[dict] = []
    for md in list(feature_dir.glob("*.md")) + list((feature_dir / "reports").glob("*.md")):
        text = md.read_text("utf-8", errors="replace")
        m = re.search(r"^## UNKNOWN Summary\n((?:.*\n)+?)(?:^## |\Z)", text, re.M)
        if not m:
            continue
        for row in m.group(1).splitlines():
            if "|" not in row or row.strip().lower().startswith("id |"):
                continue
            cols = [c.strip() for c in row.split("|") if c.strip()]
            if len(cols) >= 6 and cols[-1].lower().startswith("high"):
                high_rows.append({"doc": str(md), "row": row})
    return (len(high_rows) == 0, high_rows)


def build_bundle(feature_id: str, allow_tokens: bool) -> None:
    feature_dir = ROOT / "features" / feature_id
    reports_dir = feature_dir / "reports"
    # Gates
    if not verify_headers(feature_dir, feature_id):
        stop("Missing/invalid headers or schema_ref URN", "Fix header fields and canonical URN in all feature docs", f"{feature_dir}/*.md {reports_dir}/*.md")
        raise SystemExit(5)
    if not acceptance_vs_required(feature_dir):
        stop("Acceptance↔required mapping incomplete", "Ensure all required keys are mapped in intent_card.md", f"{feature_dir}/intent_card.md | {feature_dir}/output_contract.schema.json")
        raise SystemExit(6)
    if not concurrency_tuple_ok(feature_dir):
        stop("Missing/inconsistent concurrency tuple", "Add Concurrency Targets/Budget sections and concurrency_targets in schema", f"{feature_dir}/intent_card.md | {feature_dir}/action_budget.md | {feature_dir}/output_contract.schema.json")
        raise SystemExit(7)
    if not leakage_ok(feature_dir):
        stop("Prompt leakage/forbidden patterns detected", "Remove meta-prompt text from generated docs", f"{feature_dir}/*.md {reports_dir}/*.md")
        raise SystemExit(8)
    ok_unknowns, highs = unknowns_policy_ok(feature_dir)
    if not ok_unknowns:
        rows = "\n".join(f"{r['doc']}: {r['row']}" for r in highs)
        stop("High-impact UNKNOWNs present", "Resolve or downgrade High-impact unknowns", rows)
        raise SystemExit(9)

    # Bundle
    commit = git_commit()
    date_str = utc_date()
    schema = json.loads((feature_dir / "output_contract.schema.json").read_text("utf-8"))
    schema_ver = schema.get("version", "0.0.0")
    bundle_root = ROOT / "final_feature_documents"
    bundle_name = f"{feature_id}-{schema_ver}-{date_str}-{commit}"
    dest = bundle_root / bundle_name
    tmp = bundle_root / f".tmp.{bundle_name}.{os.getpid()}"
    shutil.rmtree(tmp, ignore_errors=True)
    tmp.mkdir(parents=True, exist_ok=True)

    # Copy docs
    for rel in REQUIRED_DOCS:
        shutil.copy2(feature_dir / rel, tmp / rel)
    (tmp / "reports").mkdir(parents=True, exist_ok=True)
    shutil.copy2(reports_dir / "creation_run.md", tmp / "reports" / "creation_run.md")
    for opt in ("manual_tests.md", "chaos_results.md", "metrics_snapshot.json"):
        if (reports_dir / opt).exists():
            shutil.copy2(reports_dir / opt, tmp / "reports" / opt)
    # Implementation brief
    final_doc_rel: str
    if (reports_dir / "implementation_brief.md").exists():
        shutil.copy2(reports_dir / "implementation_brief.md", tmp / "reports" / "implementation_brief.md")
        final_doc_rel = "reports/implementation_brief.md"
    else:
        final_doc_rel = "final_implementation_brief.md"
        with (tmp / final_doc_rel).open("w", encoding="utf-8") as f:
            f.write(f"feature_id: {feature_id}\n")
            f.write("doc_type: governance.implementation_brief\n")
            f.write(f"schema_ref: urn:automatr:schema:capsule:{feature_id}:governance.implementation_brief:v1@1.0.0\n")
            f.write("version: 1.0.0\n")
            f.write(f"updated: {dt.date.today().isoformat()}\n\n")
            f.write("## Goal\n\n")
            f.write((feature_dir / "vision.md").read_text("utf-8", errors="replace"))
            f.write("\n\n## Context\n\n")
            f.write((feature_dir / "exploration.md").read_text("utf-8", errors="replace"))
            f.write("\n\n## Actions\n\n")
            f.write((feature_dir / "action_budget.md").read_text("utf-8", errors="replace"))
            f.write("\n\n## Constraints\n\n")
            f.write((feature_dir / "observability_slos.md").read_text("utf-8", errors="replace"))
            f.write("\n\n## Contract Excerpt\n\n")
            f.write(json.dumps({"required": schema.get("required"), "properties": list((schema.get("properties") or {}).keys())}, indent=2))
            f.write("\n\n## Test Plan\n\n")
            f.write((feature_dir / "manual_tests.md").read_text("utf-8", errors="replace"))
            f.write("\n\n## Risks/Unknowns\n\n")
            for src in ("exploration.md", "intent_card.md"):
                txt = (feature_dir / src).read_text("utf-8", errors="replace")
                m = re.search(r"^## UNKNOWN Summary\n((?:.*\n)+?)(?:^## |\Z)", txt, re.M)
                if m:
                    f.write("## UNKNOWN Summary\n")
                    f.write(m.group(1))

    # Size policy: split if needed
    brief = tmp / final_doc_rel
    words = len(re.findall(r"\w+", brief.read_text("utf-8", errors="replace")))
    if words > 2133 and not allow_tokens:
        appx = tmp / "appendix.md"
        text = brief.read_text("utf-8", errors="replace")
        mid = len(text) // 2
        brief.write_text(text[:mid] + "\n\n[See appendix](appendix.md)\n", encoding="utf-8")
        appx.write_text(text[mid:] + f"\n\n[Back to final brief]({final_doc_rel})\n", encoding="utf-8")

    # Manifest and summary
    manifest = tmp / "manifest.json"
    summary = tmp / "SUMMARY.txt"
    bundle_paths = []
    hashes = {}
    for path in tmp.rglob("*"):
        if path.is_file():
            rel = str(path.relative_to(tmp))
            bundle_paths.append(rel)
            hashes[rel] = sha256_file(path)
    manifest.write_text(json.dumps({
        "feature_id": feature_id,
        "schema_ref": schema.get("$id", ""),
        "schema_semver": schema_ver,
        "repo_commit": commit,
        "created_utc": utc_iso(),
        "source_paths": [str((feature_dir / d)) for d in REQUIRED_DOCS] + [str(reports_dir / r) for r in ("creation_run.md", "manual_tests.md", "chaos_results.md", "metrics_snapshot.json") if (reports_dir / r).exists()] + [str(feature_dir / final_doc_rel)],
        "bundle_paths": bundle_paths,
        "hashes": hashes,
        "gates": {
            "identity": "PASS",
            "acceptance_to_required": "PASS",
            "concurrency_tuple": "PASS",
            "leakage_forbidden": "PASS",
            "size_policy": "PASS" if (words <= 2133 or allow_tokens) else "WARN",
            "unknowns_policy": "PASS",
            "nothing_breaks": "PASS",
        },
        "approvals": {"allow_gt_1600_tokens": bool(allow_tokens)},
        "unknowns_summary": [],
        "final_doc": final_doc_rel,
        "notes": ""
    }, indent=2), encoding="utf-8")
    summary.write_text(
        f"Final doc: {final_doc_rel}\nCommit: {commit}\nSchema: {schema_ver}\n"
        f"Gates: identity=PASS, acceptance_to_required=PASS, concurrency_tuple=PASS, leakage=PASS, size={'PASS' if (words<=2133 or allow_tokens) else 'WARN'}, unknowns=PASS, nothing_breaks=PASS\n"
        "Unknowns: High=0, Moderate/Low=(see docs)\nNext: Hand this folder to your LLM/codegen\n",
        encoding="utf-8",
    )

    # Atomic move
    if dest.exists():
        shutil.rmtree(dest)
    tmp.rename(dest)

    # Append verification record
    rec = ROOT / "capsule" / "reports" / "final_bundle_verification.md"
    rec.parent.mkdir(parents=True, exist_ok=True)
    with rec.open("a", encoding="utf-8") as f:
        f.write(f"{utc_iso()} | feature_id: {feature_id} | commit: {commit} | bundle: /{dest.as_posix()} | gates: identity=PASS, acceptance_to_required=PASS, concurrency_tuple=PASS, leakage=PASS, size={'PASS' if (words<=2133 or allow_tokens) else 'WARN'}, unknowns=PASS, nothing_breaks=PASS | final: {final_doc_rel} | size_approval: {allow_tokens} | rationale: packaged\n")

    print(f"FINAL BUNDLE CREATED: /final_feature_documents/{bundle_name}/final_doc => {final_doc_rel}")


def main():
    ns = parse_args()
    feature_id = ns.feature_id
    if not kebab_ok(feature_id):
        stop("Missing or invalid feature_id", 'Provide a kebab-case feature_id (e.g., "user-profile-sync")', f"/features/{feature_id}/")
        raise SystemExit(3)
    feature_dir = ROOT / "features" / feature_id
    reports_dir = feature_dir / "reports"
    missing = []
    if not feature_dir.exists():
        missing.append(str(feature_dir))
    if not reports_dir.exists():
        missing.append(str(reports_dir))
    for rel in REQUIRED_DOCS:
        if not (feature_dir / rel).exists():
            missing.append(str(feature_dir / rel))
    if missing:
        stop("Missing required files/directories", "Create the missing paths and try again", "\n".join(missing))
        raise SystemExit(4)

    # Optional: run validator for extra assurance
    _ = run_validator(feature_id)
    build_bundle(feature_id, ns.allow_tokens == "yes")


if __name__ == "__main__":
    main()

